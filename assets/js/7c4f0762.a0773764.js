"use strict";(self.webpackChunkucl_artificial_intelligence_society=self.webpackChunkucl_artificial_intelligence_society||[]).push([[9544],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>m});var n=a(7294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),h=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},c=function(e){var t=h(e.components);return n.createElement(l.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=h(a),p=i,m=u["".concat(l,".").concat(p)]||u[p]||d[p]||o;return a?n.createElement(m,r(r({ref:t},c),{},{components:a})):n.createElement(m,r({ref:t},c))}));function m(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:i,r[1]=s;for(var h=2;h<o;h++)r[h]=a[h];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},3429:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>h});var n=a(7462),i=(a(7294),a(3905));const o={slug:"ai-safety-debate",title:"Is AI an existential risk?",authors:["charlie"],tags:["ai safety","debate"]},r=void 0,s={permalink:"/blog/ai-safety-debate",source:"@site/blog/2024-03-17-ai-safety-debate.md",title:"Is AI an existential risk?",description:"AI Safety Debate",date:"2024-03-17T00:00:00.000Z",formattedDate:"March 17, 2024",tags:[{label:"ai safety",permalink:"/blog/tags/ai-safety"},{label:"debate",permalink:"/blog/tags/debate"}],readingTime:5.51,hasTruncateMarker:!1,authors:[{name:"Charlie Harrison",title:"Head of Content",imageURL:"/img/committee/charlie_harrison.jpg",key:"charlie"}],frontMatter:{slug:"ai-safety-debate",title:"Is AI an existential risk?",authors:["charlie"],tags:["ai safety","debate"]},nextItem:{title:"Welcome to the UCL AI Society",permalink:"/blog/welcome"}},l={authorsImageUrls:[void 0]},h=[{value:"AI Safety Debate",id:"ai-safety-debate",level:2},{value:"Why talk about AI safety?",id:"why-talk-about-ai-safety",level:3},{value:"The debate",id:"the-debate",level:3},{value:"(Dis)agreements!",id:"disagreements",level:3}],c={toc:h},u="wrapper";function d(e){let{components:t,...a}=e;return(0,i.kt)(u,(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"ai-safety-debate"},"AI Safety Debate"),(0,i.kt)("p",null,"It was a real privilege to host, alongside two of the best societies at UCL \u2013 AI Society, and UCL Effective Altruism \u2013 our AI Safety Debate, on the topic of, \u201cIs AI an existential risk?\u201d"),(0,i.kt)("p",null,"\ud83c\udfac A full recording can be found ",(0,i.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=mcozzJbLbZI"},"here"),", if you want to watch the whole thing."),(0,i.kt)("h3",{id:"why-talk-about-ai-safety"},"Why talk about AI safety?"),(0,i.kt)("p",null,"I believed it was important to host this debate because I think this question is, potentially, highly important, but also, one which I have deep uncertainties about. Many AI experts like Geoffrey Hinton think that AI should be considered just as risky as pandemics or nuclear war, and that we need to slow down, or pause its development. Others, like Melanie Mitchell, believe that the risks are \u201calmost vanishingly small\u201d. The stakes of the question for humanity warrant a serious (and long, in my opinion) conversation about the respective arguments\u2019 merits."),(0,i.kt)("p",null,"After this prelude, my wonderful colleagues Ivana and Maja introduced the speakers. We were lucky enough to have ",(0,i.kt)("a",{parentName:"p",href:"https://www.linkedin.com/in/reuben-adams-10031b180/?originalSubdomain=uk"},"Reuben Adams")," and ",(0,i.kt)("a",{parentName:"p",href:"https://www.cs.rhul.ac.uk/~chrisw/"},"Chris Watkins")," arguing for the \u201cdoomer\u201d side (as we referred to it, in the WhatsApp group chat). Reuben is a UCL AI PhD student, and host of the wonderful ",(0,i.kt)("a",{parentName:"p",href:"https://www.ucl.ac.uk/ai-centre/steering-ai-podcast"},"\u2018Steering AI\u2019 Podcast"),". Chris is a professor at Royal Holloway, and a prominent thinker in the \u2018Reinforcement Learning\u2019 field. For the \u2018risk-skeptical side\u2019 (the \u2018anti-doomer\u2019 side??), was ",(0,i.kt)("a",{parentName:"p",href:"https://www.linkedin.com/in/jack-stilgoe-2a6187a/?originalSubdomain=uk"},"Jack Stilgoe")," and ",(0,i.kt)("a",{parentName:"p",href:"https://www.linkedin.com/in/kenneth-cukier-9ab56335/?originalSubdomain=uk"},"Kenn Cukier"),". Jack is a UCL professor, on \u201chome-turf\u201d as he said, lecturing in Science and Technology Studies (STS), and works closely with UK Research and Innovation on the \u201cResponsible AI Program\u201d. Kenn is a Deputy Executive Editor at The Economist magazine, and hosts the weekly tech podcast, \u201cBabbage\u201d . ",(0,i.kt)("a",{parentName:"p",href:"https://uk.linkedin.com/in/tomough"},"Tom Ough"),", a freelance journalist, who\u2019s written various pieces about existential risk, including in Prospect Magazine, was moderating. Ivana encouraged our audience to consider how the lack of demographic diversity on the panel could systematically bias the conversation, which (as you\u2019ll read) came up in discussions."),(0,i.kt)("p",null,"At face value, the 4 speakers seemed to argue distinctly opposing points of view. I will briefly give my best effort at summarizing their views, in the order in which they spoke. After, I look to set out promising areas of agreement amongst all the panelists."),(0,i.kt)("h3",{id:"the-debate"},"The debate"),(0,i.kt)("p",null,"Reuben opened the debate. He argued that the new paradigm of deep-learning presents a distinctly new category of AI risk: we are building ever-more intelligent \u2018black-boxes\u2019, with novel capabilities we cannot predict. Once there is a \u201csecond species of intelligence\u201d that rivals our own, we are completely ignorant about what will follow. Our current tools for controlling AI systems, like \u2018RLHF\u2019, are woefully inadequate even at present, and won\u2019t \u2018scale up\u2019 with increasing AI progress. What follows from all this? \u201cI don\u2019t understand how you can confidently say that this doesn\u2019t end badly.\u201d"),(0,i.kt)("p",null,"After Reuben, came Jack. He eased his way into his argument with several cool anecdotes. (From Reuben\u2019s speech, the day after Earnest Rutherford denied the feasibility of nuclear energy, in September 1933, Leo Szilard conceived of the nuclear chain reaction. Jack said that the concept came to Szilard by Russell Square Station. Go there if you want to conceive of the next big thing). Anyway. Back to the seriousness. Rogue AI scenarios are implausible and belong in science fiction. Instead, \u201cthe idea of existential risk from AI is a form of displacement activity\u201d, from other more pressing concerns, like the disempowerment of workers or marginalization of minorities. These are the risks that deserve regulators\u2019 attention. A more interesting question, for Jack, is why people are drawn towards believing these risks: perhaps peoples\u2019 positionality, or for some technologists their self-interest. AI is a tool like any other, in that it\u2019s \u201call about power\u201d, so \u201cwe shouldn\u2019t be worrying about what robots will do to humanity, instead we should worry about what some people will do to other people\u201d."),(0,i.kt)("p",null,"After Jack, there was Chris. From his perspective, the algorithmic breakthroughs that enabled ChatGPT are pedestrian. His MSc students are already implementing the \u201ctransformer\u201d architecture for their coursework, the major breakthrough behind large language models like ChatGPT. Given that tens of thousands of people and 11 figure sums are being directed towards AI, we have no reason to believe that further breakthroughs won\u2019t occur. Instead, we should expect a future of open-ended cognitive advancement. This unknowable future is \u201cbehind a veil\u201d. While we aren\u2019t necessarily destined for doom, there are plausible \u201cside-roads\u201d that lead towards it, in particular AI-enabled authoritarian regimes."),(0,i.kt)("p",null,"Finally, it was Kenn\u2019s turn. \u201cThis is bleak!\u201d, he started with. Whilst the risks from AI are serious, they won\u2019t scale to an \u2018existential catastrophe\u2019. Existing alignment techniques like RLHF put humans in the loop, and will obstruct any \u2018intelligence explosion\u2019. Humans are unlikely to cede control of political power or nuclear missile systems to AI. Among different possible futures, we can design \u201clove\u201d into AI. In contrast, misuse risks do seem concerning. Kenn was anxious when news broke in 2022 that AI had developed 40,000 toxic chemicals in 6 hours. However, there is nuance here. The threat model of \u2018misuse risks\u2019 from bad actors already exists today. Lethal Autonomous Weapons may make warfare less brutal. So, let\u2019s not be defeatist, and instead focus on \u201cexistential solutions\u201d."),(0,i.kt)("h3",{id:"disagreements"},"(Dis)agreements!"),(0,i.kt)("p",null,"A key disagreement for the participants seemed to be: Reuben and Chris seemed to acknowledge that exact pathways to catastrophe are unknowable \u2013 and would be analogous to bonobos trying to predict how they would be outcompeted by humans. Kenn, and particularly Jack, emphasized this point, and suggested that the \u2018rogue AI story\u2019 parallels science fiction. Reuben/Chris seem to bite the bullet."),(0,i.kt)("p",null,"However, amongst these disagreements, there were several areas of agreement, which questions from our moderator, Tom, helped to elucidate:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Proactive oversight/regulation of AI systems today is necessary to guard against present-day harms, like misinformation."),(0,i.kt)("li",{parentName:"ul"},"Careful evaluations of AI models is an area of potential common ground between those concerned about \u2018near-term\u2019 and \u2018long-term risks\u2019 from AI."),(0,i.kt)("li",{parentName:"ul"},"AI represents a new (potentially transformative) era for humanity"),(0,i.kt)("li",{parentName:"ul"},"Predicting exactly how the future will unfold is nigh on impossible; speculation about how exactly AI harms might scale to catastrophe or even extinction is very difficult to conceive precisely."),(0,i.kt)("li",{parentName:"ul"},"AI is likely to be a \u201cforce-multiplier\u201d and may enable bad actors to do worse things")),(0,i.kt)("p",null,"On these points, and others, I think the speakers realized that their worldviews were closer than they might have expected."),(0,i.kt)("p",null,"I am very grateful to Ivana, Maja, Asmita for helping with the organizing of the event, and to Andrzej and Yadong for helping with the filming."))}d.isMDXComponent=!0}}]);