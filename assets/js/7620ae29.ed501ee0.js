"use strict";(self.webpackChunkucl_artificial_intelligence_society=self.webpackChunkucl_artificial_intelligence_society||[]).push([[3169],{3905:(e,t,r)=>{r.d(t,{Zo:()=>u,kt:()=>d});var n=r(7294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var l=n.createContext({}),c=function(e){var t=n.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},u=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},f=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=c(r),f=a,d=p["".concat(l,".").concat(f)]||p[f]||m[f]||o;return r?n.createElement(d,i(i({ref:t},u),{},{components:r})):n.createElement(d,i({ref:t},u))}));function d(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=r.length,i=new Array(o);i[0]=f;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:a,i[1]=s;for(var c=2;c<o;c++)i[c]=r[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,r)}f.displayName="MDXCreateElement"},6727:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var n=r(7462),a=(r(7294),r(3905));const o={sidebar_position:10},i="8: Recurrent Neural Networks",s={unversionedId:"tutorials/2024-2025/rnns",id:"tutorials/2024-2025/rnns",title:"8: Recurrent Neural Networks",description:"Date: 4th December 2024",source:"@site/our-initiatives/tutorials/2024-2025/rnns.md",sourceDirName:"tutorials/2024-2025",slug:"/tutorials/2024-2025/rnns",permalink:"/our-initiatives/tutorials/2024-2025/rnns",draft:!1,tags:[],version:"current",sidebarPosition:10,frontMatter:{sidebar_position:10},sidebar:"docsSidebar",previous:{title:"7: Generative Visual Computing",permalink:"/our-initiatives/tutorials/2024-2025/visual-computing-2"},next:{title:"9: Introduction to Transformers",permalink:"/our-initiatives/tutorials/2024-2025/intro_to_transformers"}},l={},c=[],u={toc:c},p="wrapper";function m(e){let{components:t,...r}=e;return(0,a.kt)(p,(0,n.Z)({},u,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"8-recurrent-neural-networks"},"8: Recurrent Neural Networks"),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Date: 4th December 2024")),(0,a.kt)("p",null,"\ud83d\udca1 ",(0,a.kt)("strong",{parentName:"p"},"Recurrent Neural Networks (RNNs)")," are a class of models designed to handle sequential data, such as ",(0,a.kt)("strong",{parentName:"p"},"time series")," or ",(0,a.kt)("strong",{parentName:"p"},"language"),", by using ",(0,a.kt)("strong",{parentName:"p"},"feedback loops")," to maintain ",(0,a.kt)("strong",{parentName:"p"},"context")," over time. This week, we will explore the fundamentals of RNNs, the challenges of training them\u2014especially backpropagation through time\u2014and the introduction of variants like ",(0,a.kt)("strong",{parentName:"p"},"Long Short-Term Memory (LSTM)")," networks that better capture ",(0,a.kt)("strong",{parentName:"p"},"long-term dependencies"),". We will briefly mention contrast these approaches with ",(0,a.kt)("strong",{parentName:"p"},"transformers"),", which have largely replaced RNNs and LSTMs in state-of-the-art applications by using self-attention mechanisms to model sequence elements in parallel, ultimately offering a broader perspective on modern sequence modeling techniques.\ud83d\udca1"),(0,a.kt)("p",null,"You can access our ",(0,a.kt)("strong",{parentName:"p"},"demonstration notebook")," here: \ud83d\udcd8 ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/UCLAIS/ml-tutorials-season-5/blob/main/week-8/rnn.ipynb"},(0,a.kt)("strong",{parentName:"a"},"Tutorial 8 Notebook"))),(0,a.kt)("p",null,"You can access our ",(0,a.kt)("strong",{parentName:"p"},"slides")," here: \ud83d\udcbb ",(0,a.kt)("a",{parentName:"p",href:"https://www.canva.com/design/DAGSEPaNv_I/RpD2FqJCqnRyZxwa_cvsGQ/view?utm_content=DAGSEPaNv_I&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h053c9bd49f"},(0,a.kt)("strong",{parentName:"a"},"Tutorial 8 Slides"))))}m.isMDXComponent=!0}}]);